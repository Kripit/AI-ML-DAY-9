{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ecd25b",
   "metadata": {},
   "source": [
    "# RNN \n",
    "\n",
    "- It is a neural network that handles the sequential data (e.g., time-series , stock prices) \n",
    "\n",
    "- Normal neural netowrk takes one input at a time but RNN remembers the previous step too it works as a memory , this process data with time , like trend of stok prices\n",
    "\n",
    "### RNN Equation-> \n",
    "\n",
    "#### - At every time step 't' , we calculate hidden state of RNN h_t\n",
    "\n",
    "- $h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)$\n",
    "\n",
    "1. x_t : Current Input (e.g., stok price at time t)\n",
    "\n",
    "2. h_{t-1}: previous hidden state(memory)\n",
    "\n",
    "3. W_{xh} : Hidden to Hidden weight matrix \n",
    "\n",
    "4.  W_{hh} : hidden to hidden weight matric\n",
    "\n",
    "5.  b_h : bias\n",
    "\n",
    "6. tanh: Activation function (-1 to 1 range )\n",
    "\n",
    "7. output o_t \n",
    "\n",
    "- $o_t = W_{ho} \\cdot h_t + b_o$\n",
    "\n",
    "- problem with RNN is vaishing / exploding gradients , whenever the sequence is long (e.g, 50 days) , by tanh or matrix multiplication the gradients becomes small or large , that stops the learning rate , thats why we use then GRU and  LSTM\n",
    "\n",
    "GRU (Gated Recurrent Unit):\n",
    "GRU simpler LSTM hai, jo 2 gates use karta hai:\n",
    "\n",
    "Update Gate (z_t):\n",
    "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$$\n",
    "Yeh decide karta hai ki kitna naya update karna hai.\n",
    "Reset Gate (r_t):\n",
    "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$$\n",
    "Yeh peechli memory ko kitna bhoolna hai batata hai.\n",
    "New memory:\n",
    "$$h_t = (1 - z_t) \\cdot h_{t-1} + z_t \\cdot \\tilde{h_t}$$\n",
    "Jahan $\\tilde{h_t} = \\tanh(W_h \\cdot [r_t \\cdot h_{t-1}, x_t])$.\n",
    "\n",
    "\n",
    "BiRNN:\n",
    "Yeh dono taraf (past aur future) se data dekhta hai. Hidden state:\n",
    "$$h_t = [h_t^\\rightarrow, h_t^\\leftarrow]$$\n",
    "Yeh stock trends ke liye useful hai.\n",
    "Temporal Attention:\n",
    "Har time step ko weight deta hai:\n",
    "$$\\alpha_t = \\frac{\\exp(e_t)}{\\sum \\exp(e_t)}, \\quad e_t = v^T \\cdot \\tanh(W_h \\cdot h_t)$$\n",
    "Yeh important days (e.g., market crash) pe focus karta hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de007466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler  import CosineAnnealingLR , ReduceLROnPlateau\n",
    "import yfinance as yf\n",
    "import pandas as pd \n",
    "import os \n",
    "import warnings\n",
    "from datetime import datetime ,timedelta\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error , mean_squared_error , r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc68468",
   "metadata": {},
   "source": [
    "## ðŸ“˜ Advanced Imports & Concepts\n",
    "\n",
    "### ðŸ§® FP16 vs FP32\n",
    "- **FP32 (32-bit floating point)**  \n",
    "  - Default precision in ML.  \n",
    "  - Very accurate but slower and uses more memory.  \n",
    "\n",
    "- **FP16 (16-bit floating point)**  \n",
    "  - Half precision, faster, and uses less memory.  \n",
    "  - Risk of underflow (tiny numbers become `0`) or overflow (big numbers become `âˆž`).  \n",
    "\n",
    "---\n",
    "\n",
    "### âš¡ `autocast`\n",
    "- Automatically switches between FP16 and FP32 during training.  \n",
    "- Uses FP16 where itâ€™s safe (fast) and FP32 where stability is needed (accurate).  \n",
    "- Improves training speed without losing much accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "### âš–ï¸ `GradScaler`\n",
    "- Prevents gradient underflow when training in FP16.  \n",
    "- Scales the loss before backpropagation, then rescales updates.  \n",
    "- Makes FP16 training as stable as FP32, but much faster.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“‰ Learning Rate Schedulers\n",
    "- **CosineAnnealingLR** â†’ Learning rate follows a cosine wave (fast â†’ slow â†’ fast).  \n",
    "- **ReduceLROnPlateau** â†’ Lowers learning rate when validation loss stops improving.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Metrics\n",
    "- **MAE (Mean Absolute Error)** â†’ Average absolute difference between predictions and actual values.  \n",
    "- **MSE (Mean Squared Error)** â†’ Squares errors â†’ penalizes big mistakes more.  \n",
    "- **RÂ² Score** â†’ How well predictions explain variance in data (1 = perfect).  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ˆ Finance Tools\n",
    "- **`yfinance`** â†’ Downloads stock market data (prices, volume, etc.) from Yahoo Finance.  \n",
    "- **`MinMaxScaler`** â†’ Normalizes data to range [0,1] for stable training.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ› ï¸ Data Handling\n",
    "- **`Dataset`** â†’ Defines how to load & preprocess data.  \n",
    "- **`DataLoader`** â†’ Creates batches, shuffles, and feeds data to the model.  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“‘ Logging & Warnings\n",
    "- **`logging`** â†’ Professional alternative to `print()`, saves info to log files.  \n",
    "- **`warnings.filterwarnings(\"ignore\")`** â†’ Hides unnecessary warnings (like deprecation messages).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae9f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logging.basicConfig(level = logging.INFO , format = \n",
    "                    '%(asctime)s - %(name)s - %(levelname)s - %(message)s ',\n",
    "                    handlers = [logging.StreamHandler(), logging.FileHandler(\"stock_predictions.log\")],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f28f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Data parameters\n",
    "        self.data_dir = \"./stock_data\"\n",
    "        self.sequence_length = 60  # 60 days of historical data for predicton\n",
    "        self.predict_steps = 1  # single-step prediction (easier to validate )\n",
    "\n",
    "        # training parameters - desgined to prevent overfitting\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 32  # larger batch size for stable gradient\n",
    "        self.patience = 15  # early stopping patience\n",
    "\n",
    "        # Model architecture parameters\n",
    "        self.hidden_size = 64  # 128 might cause overfitting so 128\n",
    "        self.num_layers = 2\n",
    "        self.dropout = 0.3\n",
    "\n",
    "        # Optimizations parameters\n",
    "        self.lr = 0.001\n",
    "        self.weight_decay = 1e-4\n",
    "        self.clip_grad_norm = 1.0\n",
    "\n",
    "        # file paths\n",
    "        self.model_path = \"best_Stock_rnn_model.pt\"\n",
    "        self.scaler_path = \"stock_scaler.pkl\"\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            os.makedirs(os.path.join(self.data_dir, split), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b417ca2",
   "metadata": {},
   "source": [
    "### âš™ï¸ Model & Optimization Hyperparameters\n",
    "\n",
    "#### ðŸ—ï¸ Model Parameters\n",
    "- **`self.num_layers = 2`**  \n",
    "  Number of stacked layers in the model (e.g., 2 LSTM layers).  \n",
    "  â†’ More layers = higher capacity, but risk of overfitting/slow training.  \n",
    "\n",
    "- **`self.dropout = 0.3`**  \n",
    "  Probability of \"dropping\" (ignoring) neurons during training.  \n",
    "  â†’ Prevents overfitting by making the network less dependent on specific neurons.  \n",
    "\n",
    "---\n",
    "\n",
    "#### âš¡ Optimization Parameters\n",
    "- **`self.lr = 0.001`**  \n",
    "  Learning rate â†’ how fast weights update per step.  \n",
    "  - Too high â†’ unstable training.  \n",
    "  - Too low â†’ very slow learning.  \n",
    "\n",
    "- **`self.weight_decay = 1e-4`**  \n",
    "  L2 regularization â†’ adds penalty for large weights.  \n",
    "  â†’ Helps reduce overfitting.  \n",
    "\n",
    "- **`self.clip_grad_norm = 1.0`**  \n",
    "  Gradient clipping â†’ prevents gradients from becoming too large (exploding gradients).  \n",
    "  â†’ Keeps training stable, especially in RNNs/LSTMs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the dataset , stock dataset for train test val\n",
    "class StockDataDownloader:\n",
    "    \"\"\"\n",
    "    Downloads and preprocesses stock data with proper train , test , val splits\n",
    "    includes data normalization and feature engineering\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    def download_stock_data(self, symbol=\"AAPL\", years=5):\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=years * 365)\n",
    "\n",
    "        logger.info(f\"Downloading {symbol}  data from {start_date.date()} to {end_date.date()}\")\n",
    "\n",
    "        try:\n",
    "            stock_data = yf.download(\n",
    "                symbol,\n",
    "                start=start_date.strftime(\"%Y-%m-%d\"),\n",
    "                end=end_date.strftime(\"%Y-%m-%d\"),\n",
    "                progress=False,\n",
    "            )\n",
    "            if stock_data.empty:\n",
    "                raise ValueError(f\"No data found for symbol {symbol}\")\n",
    "\n",
    "            stock_data.reset_index(inplace=True)  # reset index to make date a column\n",
    "\n",
    "            stock_data = self.add_technical_indicators(stock_data)\n",
    "\n",
    "            raw_data_path = os.path.join(self.config.data_dir, f\"{symbol}_raw.csv\")\n",
    "            stock_data.to_csv(raw_data_path, index=False)\n",
    "\n",
    "            logger.info(f\"downloaded {len(stock_data)} days of data for {symbol}\")\n",
    "            return stock_data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading data for {symbol}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def add_technical_indicators(self, df):\n",
    "        \"\"\"\n",
    "        Adding technical indicators to improve prediction accuracy.\n",
    "        These features help the model understand market trends.\n",
    "        \"\"\"\n",
    "        # Moving averages\n",
    "        df[\"MA_5\"] = df[\"Close\"].rolling(window=5).mean()\n",
    "        df[\"MA_10\"] = df[\"Close\"].rolling(window=10).mean()\n",
    "        df[\"MA_20\"] = df[\"Close\"].rolling(window=20).mean()\n",
    "\n",
    "        df[\"Returns\"] = df[\"Close\"].pct_change()\n",
    "        # .pct_change() calculates the percentage change between the current value and the previous one\n",
    "\n",
    "        # .rolling(window=10) means we take a sliding window of 10 days. , .std() means we calculate the standard deviation of returns in that window.\n",
    "        # Standard deviation in finance = measure of volatility (risk, uncertainty, how much returns fluctuate).\n",
    "        # Higher volatility = more risky stock.\n",
    "        df[\"Volatility\"] = df[\"Returns\"].rolling(window=10).std()\n",
    "\n",
    "        df[\"Price_Range\"] = (df[\"Close\"] - df[\"Low\"]) / (df[\"High\"] - df[\"Low\"])\n",
    "\n",
    "        df[\"Volume_MA\"] = df[\"Volume\"].rolling(window=10).mean()\n",
    "\n",
    "        df.fillna(method=\"bfill\", inplace=True)\n",
    "        df.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def prepare_data_splits(self, data, symbol):\n",
    "        \"\"\"\n",
    "        Split data into train/validation/test sets with proper temporal ordering.\n",
    "        This is crucial for time series data - we can't randomly shuffle.\n",
    "        \"\"\"\n",
    "        data = data.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "        total_len = len(data)\n",
    "\n",
    "        train_len = int(total_len * 0.7)\n",
    "\n",
    "        val_len = int(total_len * 0.15)\n",
    "\n",
    "        train_data = data[:train_len].copy()\n",
    "        val_data = data[train_len : train_len + val_len].copy()\n",
    "        test_data = data[train_len + val_len :].copy()\n",
    "\n",
    "        feature_columns = [\n",
    "            \"Close\",\n",
    "            \"Volume\",\n",
    "            \"MA_5\",\n",
    "            \"MA_10\",\n",
    "            \"MA_20\",\n",
    "            \"Volatility\",\n",
    "            \"Price_Range\",\n",
    "            \"Volume_MA\",\n",
    "        ]\n",
    "\n",
    "        self.scaler.fit(train_data[feature_columns])\n",
    "\n",
    "        train_data[feature_columns] = self.scaler.transform(train_data[feature_columns])\n",
    "\n",
    "        val_data[feature_columns] = self.scaler.transform(val_data[feature_columns])\n",
    "\n",
    "        test_data[feature_columns] = self.scaler.transform(test_data[feature_columns])\n",
    "\n",
    "        train_data.to_csv(os.path.join(self.config.data_dir, \"train\", f\"{symbol}.csv\"), index=False)\n",
    "\n",
    "        val_data.to_csv(os.path.join(self.config.data_dir, \"val\", f\"{symbol}.csv\"), index=False)\n",
    "\n",
    "        test_data.to_csv(os.path.join(self.config.data_dir, \"test\", f\"{symbol}.csv\"), index=False)\n",
    "\n",
    "        import joblib\n",
    "\n",
    "        joblib.dump(self.scaler, self.config.scaler_path)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Data split completed: Train={len(train_data)}, val={len(val_data)}, Test={len(test_data)}\"\n",
    "        )\n",
    "\n",
    "        return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429ad3d",
   "metadata": {},
   "source": [
    "### ðŸ“Š Example of Volatility Calculation\n",
    "\n",
    "Suppose last 10 days returns =  \n",
    "`[0.01, -0.02, 0.03, 0.01, -0.01, 0.04, -0.02, 0.00, 0.02, -0.01]`\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Mean Return (Average)\n",
    "Formula:  \n",
    "Mean return = (Sum of all returns) Ã· (Number of returns)\n",
    "\n",
    "$$\n",
    "\\bar{R} = \\frac{0.01 + (-0.02) + 0.03 + 0.01 + (-0.01) + 0.04 + (-0.02) + 0.00 + 0.02 + (-0.01)}{10}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bar{R} = 0.005 = 0.5\\%\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Variance (How spread out the returns are)\n",
    "Formula:  \n",
    "\n",
    "$$\n",
    "Var = \\frac{(R_1 - \\bar{R})^2 + (R_2 - \\bar{R})^2 + \\dots + (R_{10} - \\bar{R})^2}{10}\n",
    "$$  \n",
    "\n",
    "This measures how much each return deviates from the mean.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Standard Deviation (Volatility)\n",
    "Formula:  \n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{Var}\n",
    "$$  \n",
    "\n",
    "This tells us the *riskiness / fluctuation* of returns.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Interpretation\n",
    "- If $\\sigma = 0.02$ â†’ Volatility = **2% â†’ Low Risk** (stable stock)  \n",
    "- If $\\sigma = 0.08$ â†’ Volatility = **8% â†’ High Risk** (more unpredictable)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ead864c",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Price, Moving Averages, and Price Range â€” A Deep Dive\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Basics of Price Action\n",
    "In stock/crypto/forex markets, every **candle/bar** (1 day, 1 hour, 5 min, etc.) has 4 main values:\n",
    "\n",
    "- **Open (O)** â†’ Price at which the period started  \n",
    "- **High (H)** â†’ Highest price reached in that period  \n",
    "- **Low (L)** â†’ Lowest price reached in that period  \n",
    "- **Close (C)** â†’ Final price at end of that period  \n",
    "\n",
    "This OHLC structure is the foundation of technical analysis. Every indicator is derived from this.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Moving Average (MA)\n",
    "A **Moving Average (MA)** is a smoothing technique. It removes price noise and gives a \"trend line\".\n",
    "\n",
    "### Formula:\n",
    "For a 20-day Simple Moving Average (SMA):\n",
    "\n",
    "\\[\n",
    "MA_{20}(t) = \\frac{C_{t} + C_{t-1} + \\dots + C_{t-19}}{20}\n",
    "\\]\n",
    "\n",
    "Where \\(C_t\\) = closing price at day \\(t\\).  \n",
    "Each new day adds the latest close and drops the oldest one â†’ hence â€œmovingâ€.\n",
    "\n",
    "### Intuition:\n",
    "- **Price > MA_20** â†’ Trend is *above average*, bullish momentum.  \n",
    "- **Price < MA_20** â†’ Trend is *below average*, bearish momentum.  \n",
    "\n",
    "So MA acts as a dynamic benchmark for strength/weakness.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Bullish vs Bearish Signal\n",
    "### Condition:\n",
    "- **If Price (Close) > MA_20 â†’ Bullish**\n",
    "  - Buyers dominate, price trends upward.  \n",
    "  - Example: If past 20-day average price = â‚¹100 and today close = â‚¹110 â†’ momentum is positive.\n",
    "\n",
    "- **If Price (Close) < MA_20 â†’ Bearish**\n",
    "  - Sellers dominate, price trends downward.  \n",
    "  - Example: MA_20 = â‚¹100, close = â‚¹90 â†’ selling pressure dominates.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Price Range\n",
    "Price Range measures **volatility** â€” the total movement within a candle.\n",
    "\n",
    "### Formula:\n",
    "\\[\n",
    "\\text{Range} = High - Low\n",
    "\\]\n",
    "\n",
    "- **High** = max price buyers paid.  \n",
    "- **Low** = min price sellers accepted.  \n",
    "- Large Range â†’ high volatility (strong battle).  \n",
    "- Small Range â†’ low volatility (calm market).  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Normalized Price Range\n",
    "To add context, we normalize the close within that dayâ€™s range.\n",
    "\n",
    "### Formula:\n",
    "\\[\n",
    "Price\\_Range = \\frac{Close - Low}{High - Low}\n",
    "\\]\n",
    "\n",
    "- Value is always **0 to 1**.  \n",
    "- If Close â‰ˆ High â†’ Price closed near the top â†’ Buyers strong.  \n",
    "- If Close â‰ˆ Low â†’ Price closed near the bottom â†’ Sellers strong.  \n",
    "- If Close â‰ˆ Middle â†’ Balance between buyers/sellers.  \n",
    "\n",
    "This works like a sentiment thermometer inside each candle.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Real-World Analogy\n",
    "- **MA as Class Average**:  \n",
    "   - If class average (past 20 tests) = 70.  \n",
    "   - If you score 80 (Price > MA) â†’ above average (bullish).  \n",
    "   - If you score 60 (Price < MA) â†’ below average (bearish).  \n",
    "\n",
    "- **Price Range as Energy Level**:  \n",
    "   - Highâˆ’Low = amount of â€œdramaâ€ in the market.  \n",
    "   - If everyone scored similarly (low range) â†’ calm.  \n",
    "   - If some scored 100, some scored 20 (high range) â†’ chaos (high volatility).  \n",
    "\n",
    "---\n",
    "\n",
    "## 7. Python Code Example\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample OHLC data\n",
    "data = {\n",
    "    \"Open\":  [100, 102, 105, 103, 106],\n",
    "    \"High\":  [105, 107, 108, 106, 110],\n",
    "    \"Low\":   [98, 101, 102, 100, 105],\n",
    "    \"Close\": [104, 106, 103, 105, 108]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 20-day Moving Average (here demo, using 3-day window)\n",
    "df[\"MA_3\"] = df[\"Close\"].rolling(window=3).mean()\n",
    "\n",
    "# Bullish/Bearish\n",
    "df[\"Signal\"] = df.apply(lambda row: \"Bullish\" if row[\"Close\"] > row[\"MA_3\"] else \"Bearish\", axis=1)\n",
    "\n",
    "# Price Range\n",
    "df[\"Range\"] = df[\"High\"] - df[\"Low\"]\n",
    "\n",
    "# Normalized Price Range\n",
    "df[\"Price_Range\"] = (df[\"Close\"] - df[\"Low\"]) / (df[\"High\"] - df[\"Low\"])\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### Output Example:\n",
    "| Open | High | Low | Close | MA_3   | Signal   | Range | Price_Range |\n",
    "|------|------|-----|-------|--------|----------|-------|-------------|\n",
    "| 100  | 105  | 98  | 104   | NaN    | Bearish? | 7     | 0.857       |\n",
    "| 102  | 107  | 101 | 106   | NaN    | Bullish? | 6     | 0.833       |\n",
    "| 105  | 108  | 102 | 103   | 104.33 | Bearish  | 6     | 0.333       |\n",
    "| 103  | 106  | 100 | 105   | 104.67 | Bullish  | 6     | 0.833       |\n",
    "| 106  | 110  | 105 | 108   | 105.33 | Bullish  | 5     | 0.75        |\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "1. **Price vs MA** â†’ trend direction (bullish/bearish).  \n",
    "2. **Range (Hâˆ’L)** â†’ volatility intensity.  \n",
    "3. **Normalized Price_Range** â†’ candle sentiment (buyers vs sellers).  \n",
    "4. These basics lead to advanced tools like **RSI, Bollinger Bands, ATR, Stochastic**.  \n",
    "\n",
    "---\n",
    "\n",
    "## 9. Advanced Insight\n",
    "- **Price > MA and Price_Range â‰ˆ 1** â†’ Very strong bullish trend.  \n",
    "- **Price < MA and Price_Range â‰ˆ 0** â†’ Very strong bearish trend.  \n",
    "- **Very small Range** â†’ Market indecision (consolidation, possible breakout).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0982c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset class for handling stock data sequences.\n",
    "    Creates sequences of historical data to predict future prices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, split_type=\"train\", sequence_length=60):\n",
    "        self.split_type = split_type\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "\n",
    "        split_dir = os.path.join(data_dir, split_type)\n",
    "\n",
    "        for filename in os.listdir(split_dir):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                filepath = os.path.join(split_dir, filename)\n",
    "                df = pd.read_csv(filepath)\n",
    "\n",
    "                sequences, targets = self._create_sequences(df)\n",
    "                self.sequences.extend(sequences)\n",
    "                self.targets.extend(targets)\n",
    "\n",
    "        self.sequences = np.array(self.sequences, dtype=np.float32)\n",
    "        self.targets = np.array(self.targets, dtype=np.float32)\n",
    "\n",
    "        logger.info(f\"Created {len(self.sequences)} sequences for {split_type} set\")\n",
    "\n",
    "    def _create_sequences(self, df):\n",
    "        \"\"\"\n",
    "        Create input sequences and corresponding targets from dataframe.\n",
    "        Each sequence contains 'sequence_length' days of historical data.\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        targets = []\n",
    "\n",
    "        feature_columns = [\n",
    "            \"Close\",\n",
    "            \"Volume\",\n",
    "            \"MA_5\",\n",
    "            \"MA_10\",\n",
    "            \"MA_20\",\n",
    "            \"Volatility\",\n",
    "            \"Price_Range\",\n",
    "            \"Volume_MA\",\n",
    "        ]\n",
    "\n",
    "        for i in range(len(df) - self.sequence_length):\n",
    "            sequence = df[feature_columns].iloc[i : i + self.sequence_length].values\n",
    "            target = df[\"Close\"].iloc[i + self.sequence_length]\n",
    "            sequences.append(sequence)\n",
    "            targets.append(target)\n",
    "\n",
    "        return sequences, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx]), torch.tensor(self.targets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130bad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedStockRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced RNN model combining RNN, GRU, and Bidirectional RNN with attention.\n",
    "    Includes multiple regularization techniques to prevent overfitting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size=8, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "        super(AdvancedStockRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # layer 1: Standar RNN for basic sequential processing\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        # Layer 2: GRU for better gradient flow ane memory\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        # Bidriectional RNN for capturing both past and future context\n",
    "        self.bi_rnn = nn.GRU(\n",
    "            hidden_size,\n",
    "            hidden_size // 2,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        # Attention mechanism for focusing on important time steps\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)  # to normalize every mini - batch's variance activations ( mean = 0 , variance = 1)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, sequence_length, input_size)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initialize hidden state\n",
    "        h0_rnn = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        h0_gru = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        h0_bi = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size // 2).to(x.device)\n",
    "\n",
    "        # Pass through the rnn layers\n",
    "        rnn_out, _ = self.rnn(x, h0_rnn)  # (batch, seq_len, hidden_size)\n",
    "        gru_out, _ = self.gru(rnn_out, h0_gru)\n",
    "        bi_out, _ = self.bi_rnn(gru_out, h0_bi)\n",
    "\n",
    "        attention_scores = self.attention(bi_out)  # (batch, seq_len, 1)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)  # (batch, seq_len, 1)\n",
    "\n",
    "        # Weughted some of hidden states\n",
    "        context_vector = torch.sum(attention_weights * bi_out, dim=1)  # (batch, hidden_size)\n",
    "\n",
    "        context_vector = self.dropout(context_vector)\n",
    "\n",
    "        if context_vector.size(0) > 1:\n",
    "            context_vector = self.batch_norm(context_vector)\n",
    "\n",
    "        output = self.fc_layers(context_vector)\n",
    "\n",
    "        return output\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.patience_counter = 0\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "        self.scheduler_cosine = CosineAnnealingLR(self.optimizer, T_max=config.epochs)\n",
    "        self.scheduler_plateau = ReduceLROnPlateau(\n",
    "            self.optimizer, mode=\"min\", factor=0.5, patience=7, verbose=True\n",
    "        )\n",
    "\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train the model for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences, targets = sequences.to(self.config.device), targets.to(self.config.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                predictions = self.model(sequences)\n",
    "                loss = self.criterion(predictions.squeeze(), targets)\n",
    "\n",
    "            # Backward pass with gradient scaling\n",
    "            self.scaler.scale(loss).backward()\n",
    "\n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip_grad_norm)\n",
    "\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        return total_loss / num_batches\n",
    "\n",
    "    def validate_epoch(self, val_loader):\n",
    "        \"\"\"Validate the model for one epoch.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in val_loader:\n",
    "                sequences, targets = sequences.to(self.config.device), targets.to(self.config.device)\n",
    "\n",
    "                with autocast():\n",
    "                    predictions = self.model(sequences)\n",
    "                    loss = self.criterion(predictions.squeeze(), targets)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "        return total_loss / num_batches\n",
    "\n",
    "    def train(self, train_loader, val_loader):\n",
    "        logger.info(\"Starting model training\")\n",
    "\n",
    "        for epoch in range(self.config.epochs):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "\n",
    "            val_loss = self.validate_epoch(val_loader)\n",
    "\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            self.scheduler_cosine.step()\n",
    "            self.scheduler_plateau.step(val_loss)\n",
    "\n",
    "            # early stopping check\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "\n",
    "                # save best model\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model_state_dict\": self.model.state_dict(),\n",
    "                        \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"val_loss\": val_loss,\n",
    "                        \"train_loss\": train_loss,\n",
    "                    },\n",
    "                    self.config.model_path,\n",
    "                )\n",
    "\n",
    "                logger.info(f\"New best model saved at epoch {epoch + 1}\")\n",
    "\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "\n",
    "            # Log progress\n",
    "            if (epoch + 1) % 10 == 0 or self.patience_counter == 0:\n",
    "                logger.info(\n",
    "                    f\"Epoch {epoch + 1}/{self.config.epochs}, \"\n",
    "                    f\"Train Loss: {train_loss:.6f}, \"\n",
    "                    f\"Val Loss: {val_loss:.6f}, \"\n",
    "                    f\"LR: {self.optimizer.param_groups[0]['lr']:.7f}\"\n",
    "                )\n",
    "\n",
    "            # Early stopping\n",
    "            if self.patience_counter >= self.config.patience:\n",
    "                logger.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "        logger.info(\"Training completed!\")\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_losses, label=\"Training Loss\", color=\"blue\")\n",
    "        plt.plot(self.val_losses, label=\"Validation Loss\", color=\"red\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Model Training History\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"training_history.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "def evaluate_model(model, test_loader, config, scaler):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in test_loader:\n",
    "            sequences, targets = sequences.to(config.device), targets.to(config.device)\n",
    "\n",
    "            with autocast():\n",
    "                pred = model(sequences)\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "                actuals.extend(targets.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions).flatten()  # flatten used to conver multi dimensional array into single dimesnional array\n",
    "    actuals = np.array(actuals).flatten()\n",
    "\n",
    "    # Denormalize predictions and actuals for meaningful metrics\n",
    "    predictions_denorm = scaler.inverse_transform(\n",
    "        np.column_stack([predictions] + [np.zeros((len(predictions), 7))])\n",
    "    )[:, 0]\n",
    "    actuals_denorm = scaler.inverse_transform(\n",
    "        np.column_stack([actuals] + [np.zeros((len(actuals), 7))])\n",
    "    )[:, 0]\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals_denorm, predictions_denorm)\n",
    "    mae = mean_absolute_error(actuals_denorm, predictions_denorm)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(actuals_denorm, predictions_denorm)\n",
    "\n",
    "    # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "    mape = np.mean(np.abs((actuals_denorm - predictions_denorm) / actuals_denorm)) * 100\n",
    "\n",
    "    logger.info(\"=== Model Evaluation Results ===\")\n",
    "    logger.info(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    logger.info(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    logger.info(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    logger.info(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "    logger.info(f\"R-squared Score: {r2:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"mape\": mape,\n",
    "        \"r2\": r2,\n",
    "        \"predictions\": predictions_denorm,\n",
    "        \"actuals\": actuals_denorm,\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function that orchestrates the entire pipeline.\n",
    "    \"\"\"\n",
    "    # Initialize configuration\n",
    "    config = Config()\n",
    "\n",
    "    # Download and prepare data\n",
    "    logger.info(\"Step 1: Downloading stock data...\")\n",
    "    downloader = StockDataDownloader(config)\n",
    "\n",
    "    # Download multiple stocks for diversified training\n",
    "    stocks = [\"AAPL\", \"GOOGL\", \"MSFT\", \"TSLA\", \"AMZN\"]\n",
    "\n",
    "    for stock in stocks:\n",
    "        try:\n",
    "            stock_data = downloader.download_stock_data(stock, years=5)\n",
    "            downloader.prepare_data_splits(stock_data, stock)\n",
    "            logger.info(f\"Successfully processed {stock}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {stock}: {str(e)}\")\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    logger.info(\"Step 2: Creating datasets...\")\n",
    "    train_dataset = StockDataset(config.data_dir, \"train\", config.sequence_length)\n",
    "    val_dataset = StockDataset(config.data_dir, \"val\", config.sequence_length)\n",
    "    test_dataset = StockDataset(config.data_dir, \"test\", config.sequence_length)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    logger.info(\"Step 3: Initializing model...\")\n",
    "    model = AdvancedStockRNN(\n",
    "        input_size=8,  # Number of features\n",
    "        hidden_size=config.hidden_size,\n",
    "        num_layers=config.num_layers,\n",
    "        dropout=config.dropout,\n",
    "    ).to(config.device)\n",
    "\n",
    "    # Log model parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "\n",
    "    # Train model\n",
    "    logger.info(\"Step 4: Training model...\")\n",
    "    trainer = ModelTrainer(model, config)\n",
    "    trainer.train(train_loader, val_loader)\n",
    "\n",
    "    # Plot training history\n",
    "    trainer.plot_training_history()\n",
    "\n",
    "    # Load best model for evaluation\n",
    "    logger.info(\"Step 5: Evaluating model...\")\n",
    "    checkpoint = torch.load(config.model_path, map_location=config.device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    # Evaluate on test set\n",
    "    import joblib\n",
    "\n",
    "    scaler = joblib.load(config.scaler_path)\n",
    "    results = evaluate_model(model, test_loader, config, scaler)\n",
    "\n",
    "    # Plot predictions vs actuals\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(results[\"actuals\"][:100], label=\"Actual Prices\", color=\"blue\", alpha=0.7)\n",
    "    plt.plot(results[\"predictions\"][:100], label=\"Predicted Prices\", color=\"red\", alpha=0.7)\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Stock Price ($)\")\n",
    "    plt.title(\"Stock Price Predictions vs Actual (First 100 Test Samples)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"predictions_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    logger.info(\"Pipeline completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
